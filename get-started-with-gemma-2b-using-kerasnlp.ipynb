{"metadata":{"colab":{"name":"get_started.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11371,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":5171}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2024 Google LLC.","metadata":{"id":"Tce3stUlHN0L"}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"tuOe1ymfHZPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/get_started\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n  </td>\n    <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/get_started.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/get_started.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n</table>","metadata":{"id":"4qxv4Sn9b8CE"}},{"cell_type":"markdown","source":"# Get started with Gemma using KerasNLP\n\nThis tutorial shows you how to get started with Gemma using [KerasNLP](https://keras.io/keras_nlp/). Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. KerasNLP is a collection of natural language processing (NLP) models implemented in [Keras](https://keras.io/) and runnable on JAX, PyTorch, and TensorFlow.\n\nIn this tutorial, you'll use Gemma to generate text responses to several prompts. If you're new to Keras, you might want to read [Getting started with Keras](https://keras.io/getting_started/) before you begin, but you don't have to. You'll learn more about Keras as you work through this tutorial.","metadata":{"id":"PXNm5_p_oxMF"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"mERVCCsGUPIJ"}},{"cell_type":"markdown","source":"### Gemma setup\n\nTo complete this tutorial, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n\n* Get access to Gemma on kaggle.com.\n* Select a Colab runtime with sufficient resources to run\n  the Gemma 2B model.\n* Generate and configure a Kaggle username and API key.\n\nAfter you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment.\n","metadata":{"id":"QQ6W7NzRe1VM"}},{"cell_type":"markdown","source":"### Set environment variables\n\nSet environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`.","metadata":{"id":"_gN-IVRC3dQe"}},{"cell_type":"code","source":"# If using Kaggle\n\"\"\"\nNote: Kaggle notebooks have a key storage feature under \nAdd-ons > Secrets, along with instructions for accessing stored keys.\n\"\"\"\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"KAGGLE_KEY\")\nsecret_value_1 = user_secrets.get_secret(\"KAGGLE_USERNAME\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T03:20:59.495683Z","iopub.execute_input":"2024-02-22T03:20:59.496089Z","iopub.status.idle":"2024-02-22T03:20:59.790757Z","shell.execute_reply.started":"2024-02-22T03:20:59.496044Z","shell.execute_reply":"2024-02-22T03:20:59.790009Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# If using Google Colab\n\"\"\"\nIn Colab, select Secrets (🔑) and add your Kaggle username and Kaggle API key. \nStore your username under the name KAGGLE_USERNAME and \nyour API key under the name KAGGLE_KEY.\n\"\"\"\nimport os\nfrom google.colab import userdata\n\n# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n# vars as appropriate for your system.\nos.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\nos.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')","metadata":{"id":"DrBoa_Urw9Vx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Install dependencies\n\nInstall Keras and KerasNLP.","metadata":{"id":"z9oy3QUmXtSd"}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"id":"UcGLzDeQ8NwN","outputId":"42fab543-a6ee-4d9d-dd86-19e95e087549","execution":{"iopub.status.busy":"2024-02-22T03:00:37.434451Z","iopub.execute_input":"2024-02-22T03:00:37.435232Z","iopub.status.idle":"2024-02-22T03:01:06.671117Z","shell.execute_reply.started":"2024-02-22T03:00:37.435200Z","shell.execute_reply":"2024-02-22T03:01:06.670220Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflowjs 4.16.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### Import packages\n\nImport Keras and KerasNLP.","metadata":{"id":"FX47AUYrXwLK"}},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"id":"ww83zI9ToPso","execution":{"iopub.status.busy":"2024-02-22T03:02:06.074800Z","iopub.execute_input":"2024-02-22T03:02:06.075187Z","iopub.status.idle":"2024-02-22T03:02:20.044751Z","shell.execute_reply.started":"2024-02-22T03:02:06.075152Z","shell.execute_reply":"2024-02-22T03:02:20.043921Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-02-22 03:02:08.193572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-22 03:02:08.193669: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-22 03:02:08.358598: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Select a backend\n\nKeras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. [Keras 3](https://keras.io/keras_3) lets you choose the backend: TensorFlow, JAX, or PyTorch. All three will work for this tutorial.","metadata":{"id":"Pm5cVOFt5YvZ"}},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".","metadata":{"id":"7rS7ryTs5wjf","execution":{"iopub.status.busy":"2024-02-22T03:02:22.874782Z","iopub.execute_input":"2024-02-22T03:02:22.875713Z","iopub.status.idle":"2024-02-22T03:02:22.880391Z","shell.execute_reply.started":"2024-02-22T03:02:22.875677Z","shell.execute_reply":"2024-02-22T03:02:22.879338Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Create a model\n\nKerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/). In this tutorial, you'll create a model using `GemmaCausalLM`, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n\nCreate the model using the `from_preset` method:","metadata":{"id":"ZsxDCbLN555T"}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n","metadata":{"id":"yygIK9DEIldp","outputId":"155c9a43-2655-4d00-fc6c-a9ec3ca24364","execution":{"iopub.status.busy":"2024-02-22T03:02:26.934030Z","iopub.execute_input":"2024-02-22T03:02:26.934864Z","iopub.status.idle":"2024-02-22T03:03:20.939532Z","shell.execute_reply.started":"2024-02-22T03:02:26.934833Z","shell.execute_reply":"2024-02-22T03:03:20.938632Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"`from_preset` instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma_2b_en\"` specifies the preset architecture: a Gemma model with 2 billion parameters. (A Gemma model with 7 billion parameters is also available. To run the larger model in Colab, you need access to the premium GPUs available in paid plans. Alternatively, you can perform [distributed tuning on a Gemma 7B model](https://ai.google.dev/gemma/docs/distributed_tuning) on Kaggle or Google Cloud.)\n\nUse `summary` to get more info about the model:","metadata":{"id":"XrAWvsU6pI0E"}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"id":"e5nEbTdApL7W","outputId":"5bb5d67f-7bd0-4662-f0d9-0af75a5997b6","execution":{"iopub.status.busy":"2024-02-22T03:03:31.892667Z","iopub.execute_input":"2024-02-22T03:03:31.893612Z","iopub.status.idle":"2024-02-22T03:03:31.927907Z","shell.execute_reply.started":"2024-02-22T03:03:31.893575Z","shell.execute_reply":"2024-02-22T03:03:31.927027Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see from the summary, the model has 2.5 billion trainable parameters.","metadata":{"id":"81KHdRYOrWYm"}},{"cell_type":"markdown","source":"## Generate text\n\nNow it's time to generate some text! The model has a `generate` method that generates text based on a prompt. The optional `max_length` argument specifies the maximum length of the generated sequence.\n\nTry it out with the prompt `\"What is the meaning of life?\"`.","metadata":{"id":"FOBW7piN5-sl"}},{"cell_type":"code","source":"gemma_lm.generate(\"How can we achieve AGI?\", max_length=68)","metadata":{"id":"aae5GHrdpj2_","outputId":"8dac954d-f125-4802-9e42-5dbe42cc67ec","execution":{"iopub.status.busy":"2024-02-22T03:09:58.812432Z","iopub.execute_input":"2024-02-22T03:09:58.813053Z","iopub.status.idle":"2024-02-22T03:10:21.748060Z","shell.execute_reply.started":"2024-02-22T03:09:58.813023Z","shell.execute_reply":"2024-02-22T03:10:21.747110Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"W0000 00:00:1708571419.136898      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'How can we achieve AGI?\\n\\nThe answer is simple: we need to build a machine that can learn from experience.\\n\\nThe problem is that we don’t know how to build such a machine.\\n\\nThe answer is simple: we need to build a machine that can learn from experience.\\n\\nThe problem is that we don'"},"metadata":{}}]},{"cell_type":"markdown","source":"Try calling `generate` again with a different prompt.","metadata":{"id":"qH0eFH_DvYwM"}},{"cell_type":"code","source":"gemma_lm.generate(\"How does NVIDIA GPU work?\", max_length=64)","metadata":{"id":"VEyTnnNGvgGG","outputId":"b6238918-4aec-4c48-bde5-1a2e72657f67","execution":{"iopub.status.busy":"2024-02-22T03:10:23.897000Z","iopub.execute_input":"2024-02-22T03:10:23.897402Z","iopub.status.idle":"2024-02-22T03:10:26.353390Z","shell.execute_reply.started":"2024-02-22T03:10:23.897372Z","shell.execute_reply":"2024-02-22T03:10:26.352362Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'How does NVIDIA GPU work?\\n\\nNVIDIA GPU is a graphics processing unit (GPU) that is used to accelerate the processing of graphics and video data. It is a specialized chip that is designed to handle complex calculations and graphics operations quickly and efficiently.\\n\\nNVIDIA GPU is made up of a number of processing cores, which'"},"metadata":{}}]},{"cell_type":"markdown","source":"If you're running on JAX or TensorFlow backends, you'll notice that the second `generate` call returns nearly instantly. This is because each call to `generate` for a given batch size and `max_length` is compiled with XLA. The first run is expensive, but subsequent runs are much faster.","metadata":{"id":"vVlCnY7Gvm7U"}},{"cell_type":"markdown","source":"You can also provide batched prompts using a list as input:","metadata":{"id":"mw5XkiHU11Ft"}},{"cell_type":"code","source":"gemma_lm.generate(\n    [\"How can we achieve AGI?\",\n     \"How does NVIDIA GPU work?\"],\n    max_length=64)","metadata":{"id":"xV6vs8_C2BGt","outputId":"754f3a1e-1c6f-4f25-880c-1380698cb272","execution":{"iopub.status.busy":"2024-02-22T03:10:51.682044Z","iopub.execute_input":"2024-02-22T03:10:51.682404Z","iopub.status.idle":"2024-02-22T03:11:11.537091Z","shell.execute_reply.started":"2024-02-22T03:10:51.682377Z","shell.execute_reply":"2024-02-22T03:11:11.536100Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"W0000 00:00:1708571468.771139      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['How can we achieve AGI?\\n\\nThe answer is simple: we need to build a machine that can learn from experience.\\n\\nThe problem is that we don’t know how to build such a machine.\\n\\nThe answer is simple: we need to build a machine that can learn from experience.\\n\\nThe problem',\n 'How does NVIDIA GPU work?\\n\\nNVIDIA GPU is a graphics processing unit (GPU) that is used to accelerate the processing of graphics and video data. It is a specialized chip that is designed to handle complex calculations and graphics operations quickly and efficiently.\\n\\nNVIDIA GPU is made up of a number of processing cores, which']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Optional: Try a different sampler\n\nYou can control the generation strategy for `GemmaCausalLM` by setting the `sampler` argument on `compile()`. By default, [`\"greedy\"`](https://keras.io/api/keras_nlp/samplers/greedy_sampler/#greedysampler-class) sampling will be used.\n\nAs an experiment, try setting a [`\"top_k\"`](https://keras.io/api/keras_nlp/samplers/top_k_sampler/) strategy:","metadata":{"id":"MaVWoSpo3XyY"}},{"cell_type":"code","source":"gemma_lm.compile(sampler=\"top_k\")\ngemma_lm.generate(\"How can we achieve AGI?\", max_length=64)","metadata":{"id":"mx55VQpN4DAK","outputId":"46bd5f37-0e7f-4655-f8cc-d8063b3ebdc4","execution":{"iopub.status.busy":"2024-02-22T03:11:36.114145Z","iopub.execute_input":"2024-02-22T03:11:36.114535Z","iopub.status.idle":"2024-02-22T03:11:58.864869Z","shell.execute_reply.started":"2024-02-22T03:11:36.114506Z","shell.execute_reply":"2024-02-22T03:11:58.863688Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"W0000 00:00:1708571516.438119      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1708571516.535977      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'How can we achieve AGI? What is a good definition for AGI? How can we measure it? How do we measure it now? How do we measure it tomorrow? Is it possible at all?\\n\\nThese are just some of the questions we will address during this 2-day workshop.\\n\\nThe'"},"metadata":{}}]},{"cell_type":"markdown","source":"While the default greedy algorithm always picks the token with the largest probability, the top-K algorithm randomly picks the next token from the tokens of top K probability.\n\nYou don't have to specify a sampler, and you can ignore the last code snippet if it's not helpful to your use case. If you'd like learn more about the available samplers, see [Samplers](https://keras.io/api/keras_nlp/samplers/).","metadata":{"id":"-okKgK4LfO0f"}},{"cell_type":"markdown","source":"## What's next\n\nIn this tutorial, you learned how to generate text using KerasNLP and Gemma. Here are a few suggestions for what to learn next:\n\n* Learn how to [finetune a Gemma model](https://ai.google.dev/gemma/docs/lora_tuning).\n* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/distributed_tuning).\n* Learn how to [use Gemma models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma).","metadata":{"id":"jBrbTYasoo-J"}}]}